1. **Overview**
核心问题：大语言模型是否能够正确地理解「三维空间中物体旋转」所带来的视觉变化？它们是否能像人类一样，在空间想象任务中体现出一致性、准确性与自我认知能力（confidence calibration）？
实验方法总览：
   - 以多张虚拟渲染的 3D 物体的照片为素材。
   - 设计多种旋转操作与问题类型
   - 测试多种 LLM 模型，并加入人类参与者作为对照组。
   - 对结果进行量化分析与模型间比较，解读数据并做出结论。

2. **Source of Pictures**
   - 渲染法：
     - 用同一套灯光与相机参数离线渲染物体视角。
     - 渲染法能直接获得精确真值角度，便于量化。

3. **Task Design**
三个难度操控维度：
- 旋转轴数量（1、2）
- 旋转角度大小（low、high）
- 物体对称性（low、high）

→ 这三个因素都是二水平，因此共有 8 个条件（2×2×2）。

1. **如何量化题目难度？**
- 越难的题目，题目的分值越高。

- **用于“分值”的一个简单做法（可选）**：先用人类组的 `difficulty_acc` 对题目做分位数分组（例如 5 组：1–5 分），然后对每个模型/被试计算加权得分 `Score = Σ(points_i · correct_i)`，从而让“做对困难题”比“做对简单题”贡献更大。

1. **测试组**
测试多个模型（GPT-5.2、Qwen、Gemini）。每道题在全新会话中运行，以避免上下文干扰。
收集模型返回每道题答案的 confidence（0~1）。目的：评估模型是否具有「自知之明」，即在不确定时能否降低信心，从而反映其元认知能力（metacognition）。有条件的话，可以计算：Calibration Curve（置信度–准确率曲线）和ECE（Expected Calibration Error）。

观察 LLM 是否展现出类似人类的「信心调整」行为（即困难题信心降低、简单题信心提高）。分析不同模型的个体差异及其「过度自信」或「低估」倾向。
