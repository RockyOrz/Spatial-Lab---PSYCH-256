## 认知科学导论 / 2025 秋季学期

### 期末项目说明

你（以及如果适用的话，你所在小组中的 1–3 名其他同学）将研究一个或多个大型语言模型（LLM，例如 ChatGPT、Claude 或 Gemini）的认知能力。

#### 项目简介

正如我们在课堂上讨论的那样，LLM 是规模巨大的神经网络，它们能够完成许多并未被明确训练去做的惊人任务。然而，我们目前既不了解它们全部的能力范围（以及局限），也不真正理解为什么它们能够（或不能够）做成这些事情。

你的任务是：选择一个你感兴趣的认知任务，考察一个或多个 LLM 在该任务上的能力。你可以使用模型的网页界面，也可以在自己熟悉的情况下使用 API。不过，这个作业并不要求你具备高级的计算机技能。

---

### 如何选择要研究的认知能力？

你可以通过三种主要方式来选择研究对象：

- 选择课堂上或指定/推荐阅读材料中讨论过的内容；
- 如果你是心理学专业的学生，可以从自己在其他心理学课程中学到的现象入手，或者参考心理学教材的目录来寻找灵感；
- 浏览心理学或认知科学期刊上的文章，找到一篇关于人类认知中新现象的论文，然后测试 LLM 是否也会表现出同样的现象。

  一些适合查阅的期刊包括（但不限于）：  
  *Cognition*、*Cognitive Science*、*Psychological Science*、*Journal of Experimental Psychology: General*、*Journal of Experimental Psychology: Learning, Memory, & Cognition*、*Judgment & Decision-Making* 和 *Journal of Experimental Social Psychology*。  
  这一选择路径在起步时可能更困难一些，但好处是：期刊文章通常会附带大量文献引用，可以帮助你撰写报告。

请注意，“认知能力”的形式多种多样，可以包括人类的成功表现，也可以包括失败表现（例如“认知偏差”）。在 Learn 上，我们放置了几篇学术论文作为示例，这些论文都尝试将 LLM 的表现与人类进行对比；其中有的在问“LLM 在这一任务上的能力是否与人类一样强？”，有的则在问“LLM 在这一任务上的偏差是否和人类一样严重？”（或者“LLM 是否以与人类相同的方式表现出偏差？”）。

例如，类比推理相关的论文一般假设人类在类比方面表现良好，并据此检验 LLM 是否同样擅长；而关于元认知的论文则假设人类往往过度自信，并进一步考察 LLM 在这一维度上是否同样“糟糕”。

---

### 我应该如何测试 LLM？

建议你先从“摸索”开始，多尝试不同的提问，看看模型会给出怎样的回答。起初你可以直接使用学术论文中的问题，但理想情况下，你还应自己构造一些新的例子，以测试模型是否具备泛化能力（例如，避免模型的训练数据被原始论文“污染”的情况）。

请尽量以“实验工作者”的视角来思考问题，围绕相似任务设计不同的变体，从而更好地探查模型在何处表现良好、在何处失败。最终，你应该使用一种尽可能系统化的方法。

在课堂上提到、并已上传到 Learn 的这篇短文，为如何为 LLM 设计认知测试提供了很多非常有价值的建议：

> Frank, M. C. (2023). Baby steps in evaluating the capacities of large language models. *Nature Reviews Psychology*, 2, 451–452.

你必须在报告中附上一个附录，完整包含你使用的所有提示（prompts）以及模型给出的所有输出。你无需在正文中逐一叙述这些内容（实际上，这么做往往会让报告变得混乱）。

---

### 关于“上下文窗口”的技术性注意事项

有一个技术问题需要特别注意：如果你在同一次对话中向 LLM 提出多个提示，模型的“上下文窗口”（即用于生成当前输出的输入 token 数量）可能会包括你之前的部分提示。因此，你需要确保自己在进行某一项任务的测试时，已经“清空”了上下文窗口，从而获得一次“干净”的测试。

如何清空上下文窗口在不同模型之间有所差异，你需要查明你所使用模型的具体操作方式。例如：

- **ChatGPT**：进入设置 → “Personalization”（个性化）→ 确认“Reference saved memories”（引用已保存记忆）和 “Reference chat history”（引用聊天记录）均已关闭。之后，新建一个对话即可清空上下文窗口。若不进行这些设置，ChatGPT 可能会访问此前保存的记忆和最近的聊天记录。
- **Claude**：新建一段对话。
- **Gemini**：新建一段对话。
- **Grok**：在 X 的设置中依次进入 Privacy and Safety → Grok Settings → 关闭 “Personalize Grok with your conversation history”（用你的对话记录个性化 Grok），并删除相关历史记录。

一般而言，你应尽量使每次测试都在一个相对独立的上下文中进行，以更真实地评估模型在该任务上的能力。

---

### 我们如何构思一个“更有野心”的项目？

在选择课题时，你应该先检查一下现有的研究文献，看看是否已经有人尝试对相似的能力进行基准测试。如果已经有类似工作，这是完全没有问题的；但在这种情况下，你需要将自己的工作置于这些研究的背景之下，说明你的方法或结果有哪些不同之处。

下面列出了一些可以让你的项目“更具野心”的方向（我们并不要求你全部做到）：

- 不局限于课堂或 Learn 上的论文，而是从更广泛的研究文献中选择任务或能力；
- 针对同一认知能力设计更多不同类型的测试，以获得收敛性证据，并检验模型的泛化能力；
- 使用更有原创性或更具创新性的测试方法，或基于对现有文献更深入的理解来设计测验；
- 设计足够多的提示/任务/项目，使你能够计算描述性统计量，甚至进行推断统计（并可能配以图表呈现），而不仅仅是做定性描述；
- 对多个不同的模型进行测试，并对它们的表现做（可能是定量的）比较；
- 任何你和小组成员能想到的、有助于“跳出框架”的创意。我们非常欢迎富有创意、巧妙且有想象力的项目；
- 不仅关注模型“是否犯错”，还要进一步探究模型“是怎么犯错的”，例如通过后续测试来分析其错误背后的机制。

当你提交项目构想简要说明（prospectus）时，我们会就这些方面给予反馈。

---

### 是否有可供参考的基准测试示例？

在 Learn 上，我们上传了若干学术论文，这些论文都是关于不同认知能力的基准测试。你完全可以将这些论文作为灵感来源，但我们并不期待你们的项目能够达到正式学术出版物的水准。

这些论文包括：

- 若干篇关于不同认知能力的论文（例如：类比、心智理论、解释能力和元认知）；
- 一系列都聚焦在同一能力（这里以“类比”为例）的论文，用来展示不同方法可能会导致不同结论。

---

### 报告评分标准

最终报告将根据以下四个方面进行评分：

1. **概念解释（20%）**  
   既然你要考察某种认知能力，就需要说明人类在这个能力上的研究现状（以及在适用情况下，已有的关于 LLM 在该能力上的研究）。请解释该能力究竟是什么，以及在人类身上被认为由哪些心理过程支撑。  
   在某些情况下，这可能意味着你需要介绍一篇或多篇对该能力已有较为详细研究的期刊论文；在另一些情况下，你所选择的能力可能尚未进入学术文献，这时你可以通过有理有据的分析，讨论这一能力在心理机制层面可能涉及什么过程，而不必大量引用文献。

2. **测试方法与执行（40%）**  
   你需要解释自己用来测试 LLM 的方法（如果你采用了人类或 LLM 研究中的既有范式，也应简要介绍相关方法）。请报告足够多的例子（以及在适用情况下的描述性统计量），以清晰展示你发现的核心结果。

3. **结果解释（25%）**  
   在你对认知能力更广泛的理论说明框架下，对这些结果进行解释：我们应该从这些结果中得出什么结论？你的方法有哪些局限？未来的研究可以如何改进这些局限？

4. **表达与写作（15%）**  
   报告应当条理清晰、语言简洁，并在段落之间形成连贯的叙述逻辑。理所当然地，报告中不应该出现拼写或语法错误。

---

### 关于在写作中使用 LLM 的说明

在本项目中，你可以在多种方面自由使用 LLM，包括在你们认为必要时用它来生成报告中的文字。然而，你必须遵守以下要求：

1. 你需要像引用其他资料一样，对所有外部信息来源进行恰当的引用；
2. 你必须在报告的附录中完整呈现所有提示（prompts）及其对应输出，包括用于帮助撰写报告正文的那些对话；
3. 你和你的小组成员需要为模型给出的所有内容负责，包括其中可能出现的事实性错误或逻辑错误。

---

### 附注：课程大纲中的“细则”

**分组**  
本项目可以选择个人完成，也可以最多四人组成小组完成。除非在有充分证据的情况下出现极不均等的工作量分配，否则同一小组内的成员将获得相同的成绩。  
需要注意的是，小组人数越多、项目就越应该在“野心程度”上有所提高（即：四人小组的预期项目规模和难度要高于个人项目）。

**篇幅**  
本项目没有硬性的字数或页数要求，但我们预计：每位组员大约需要 4 页双倍行距的内容，才能展示出足够详细、同时具有一定“野心”的项目。例如：个人项目约 4 页，小组为 4 人时，则大致在 16 页左右。

**生成式 AI 的使用**  
本项目的特殊之处在于：你可以以任意方式使用 LLM，包括在你的小组认为合适的情况下，使用 LLM 生成文本。但你必须对所有引用的内容进行规范标注，并在报告的附录中提供完整的对话记录（包括所有提示和输出）。该附录不计入正文页数，可以相当长。

**阶段性成果与提交要求**  
为了帮助你合理规划项目进度，本课程设置了若干里程碑，请务必按照课程大纲中的截止日期完成。

- **第一次截止日期（2025 年 9 月 19 日）**  
  小组中的一名成员需要给助教发送邮件，内容可以是以下三种情况之一：  
  1. 列出你们小组成员名单；  
  2. 表示希望由课程组为你分配小组；  
  3. 表示你打算独立完成项目。  
  如果你在此日期前没有联系助教，我们将默认你希望被自动分配到一个小组，并据此进行分组。

- **第二次截止日期（2025 年 10 月 24 日）**  
  小组中的一位成员需要在 Learn 上提交一份 1 页左右的项目构想简要说明（prospectus），其中应包括：  
  1. 对你的项目想法的简要概述；  
  2. 你们实施该项目的计划（例如：需要完成哪些任务、由谁来完成等）。  
  助教会就这份构想提供简短反馈，以确保你们走在正确的方向上。  
  这一构想说明占课程总评的 2.5%。

- **第三次截止日期（2025 年 12 月 15 日）**  
  小组中的一名成员需要在 Learn 上提交最终的项目报告。提交内容为一个单独的 PDF 文件，其中包含：  
  - 主体报告，详细说明你们的研究和发现；  
  - 附录部分，完整呈现你们使用的所有提示及对应的模型输出。  
  最终报告占课程总评的 22.5%。

